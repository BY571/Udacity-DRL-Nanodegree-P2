{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG-Report \n",
    "\n",
    "The chosen algorithm for the reacher environment is the Deep Deterministic Policy Gradient (DDPG) algorithm. Its part of the A2C / Actor-Critic family. That means the agent consists of two neural networks, one is the Actor. It takes the state as an input and returns two values: mu and sigmaÂ². These are the mean and the variance of a (action-) distribution. The critic network takes the state and a calculates action by the mean and variance as an input and returns the value of the Q-Value: critic_net(state, actor_net(state)) -> Q(s,a).\n",
    "\n",
    "#### Architecture:\n",
    "The network of the Actor consists of 2 fully connected (hidden) layers with a size of 400 and 300 units.\n",
    "After the first layer a batch normalization is integrated and for all except the output layer the activation funtions are relu-activation-functions. The output layer has a tanh-activation-funtion to give an output between [-1, 1].\n",
    "\n",
    "The Critic network is as well build of 2 fully connected (hidden) layers with a size of 400 and 300 units.\n",
    "As with the actor network, after the first layer there is a batch normalization. For activation function the relu function is used.\n",
    "\n",
    "#### Training\n",
    "Since the DDPG-Algorithm is a off-policy algorithm the (state,reward, action, next state)-tuples after each step are saved in a replay buffer from where later batches are sampled from to learn of the experience.\n",
    "The optimization happens every 20 steps and the optimization step is repeated 10 times.\n",
    "\n",
    "The critics loss is calculated as the MSE-loss between the expected-Q-value and the target-Q-value as equivalent Q-learning algorithms.\n",
    "The actor loss is calculated by the negative output of the critic network with the inputs of the current state and the predicted action by the actor. \n",
    "\n",
    "Both, actor and critic networks are optimized by the adam optimizer with a learning rate of 1e-3.\n",
    "After each optimization the weights of the networks are updated with a softupdate step with a tau-value of 1e-3.\n",
    "Further a gamma of 0.99 is chosen and a noise is added to the actions to improve exploration (an epsilon value to decrease the influence of the noise to the actions over time).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./Imgs/results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further improvements\n",
    "\n",
    "To improve learning the DDPG-Algorithm could be updated to a new developed D4PG-Algorithm that uses several techniques/improvements that were discovered on the value-based methods in reinforcement learning. For D4PG that would imply:\n",
    "- n-step temporal difference calculation\n",
    "- using a prioritized experience replay buffer\n",
    "- replacing the single Q-values from the critic with a probability distribution. Where the Bellman equation is replaced with the Bellman operator that displays the distribution in a simular way as the single Q-value.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
